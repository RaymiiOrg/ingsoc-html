
    <!DOCTYPE html>
    <html lang="en">
        <head>
        <title>Openstack - (Manually) migrating (KVM) Nova compute virtual machines - Raymii.org</title>
        <!-- <link href="/s/inc/css/custom-first.css" rel="stylesheet" />-->
        <link href="/s/inc/css/marx.css" rel="stylesheet"  />
        <!-- <link href="/s/inc/css/custom.css" rel="stylesheet" title="custom" />-->
        <script src="/s//inc/js/instant.js" type="text/javascript"></script> 
        <script src="/s//inc/js/toc.js" type="text/javascript"></script> 
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link type="application/opensearchdescription+xml" rel="search" href="/s/inc/opensearch.xml"/>
        <link rel="alternate" type="application/rss+xml" title="RSS Feed for Raymii.org" href="https://raymii.org/s/feed.xml" />
    </head>
    <body>
        
        <a id="top-of-page"></a>
        <main>
        <a class="skip-main" href="#main">Skip to main content</a>
            <header>
                <h1 class="headheader">
                    <a href="https://raymii.org/s/">Raymii.org <img src="/s/inc/img/resistor-50.png"></a>
                </h1>
                <small>
                  Quis custodiet ipsos custodes?<br>
                  <a href="/s/">Home</a> | 
                  <a href="/s/static/About.html">About</a> | 
                  <a href="/s/tags/all.html">All pages</a> | 
                  <a href="https://raymii.org/s/feed.xml">RSS Feed</a> | 
                  <a href="gopher://raymii.org:70">Gopher</a>
                </small>
            </header>
          

    <h2 class='headheader' id='main'>Openstack - (Manually) migrating (KVM) Nova compute virtual machines</h2><p><small>Published: 13-06-2015 | Author: Remy van Elst | <a href="Openstack_-_(Manually)_migrating_(KVM)_Nova_Compute_Virtual_Machines.txt" class="link">Text only version of this article</small></a></p><div class='ad'>
                            <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
                            <!-- voorartikel -->
                            <ins class="adsbygoogle"
                                 style="display:inline-block;width:728px;height:90px"
                                 data-ad-client="ca-pub-7993642564731324"
                                 data-ad-slot="6172324376"></ins>
                            <script>
                            (adsbygoogle = window.adsbygoogle || []).push({});
                            </script>
                        </div><br><div id="toc"><h3>Table of Contents</h3></div><hr><div id="contents"><p>This guide shows you how to migrate KVM virtual machines with the Openstack Nova
compute service, either manually or with the Openstack tooling.</p>

<p>Migrating compute instances is very usefull. It allows an administrator to free
up a compute node for maintenance/updates. It also allows an administrator to
better distribute resources between compute nodes.</p>

<p>Openstack provides a few different ways to migrate virtual machines from one
compute node to another. Each option has different requirements and
restrictions, for example:</p>

<ul>
<li>You can&#39;t live-migrate without shared storage. </li>
<li>You can&#39;t live-migrate if you have a configdrive enabled. </li>
<li>You can&#39;t select the target host if you use the nova migrate (non-live) command.</li>
</ul>

<p>Later on in this guide, I&#39;ll list the most common limitations per situation.</p>

<p>This article describes the most common migration scenario&#39;s including live and
manual migration using native linux tools.</p>

<p>You can see all my <a href="https://raymii.org/s/tags/openstack.html">Openstack related articles here</a>. For example, how to
build a <a href="https://raymii.org/s/articles/Building_HA_Clusters_With_Ansible_and_Openstack.html">High Available cluster with Ansible and Openstack</a>.</p>

<p><a href="https://www.digitalocean.com/?refcode=7435ae6b8212">If you like this article, consider sponsoring me by trying out a Digital Ocean
VPS. With this link you&#39;ll get $100 credit for 60 days). (referral link)</a></p>

<p>It is tested on an Openstack cloud running Icehouse with KVM as the only
hypervisor. If you run Xen or something else, the manual process is mostly the
same, however might require specific adaptions.</p>

<h3>Configure (live) migration</h3>

<p>There are a few things you need to configure (live) migrations on an Openstack
cloud. The Openstack documentation describes this very well, <a href="http://docs.openstack.org/admin-guide-cloud/content/section_configuring-compute-migrations.html">read that
first.</a></p>

<p>If you have a cloud configured for live migration, read on.</p>

<h3>Migration limitations</h3>

<p>Openstack has two commands specific to virtual machine migration:</p>

<ul>
<li><code>nova migrate $UUID</code></li>
<li><code>nova live-migration $UUID $COMPUTE-HOST</code></li>
</ul>

<p>The <code>nova migrate</code> command shuts down an instance to move it to another
hypervisor.</p>

<ul>
<li>The instance is down for a period of time and sees this as a regular shutdown. </li>
<li>It is not possible to specify the compute host you want to migrate the instance to. (Read on to see how you can do that the dirty way). </li>
<li>This command does not require shared storage, the migrations can take a long time. </li>
<li>The Openstack cluster chooses the target hypervisor machine based on the free resources and availability. </li>
<li>The migrate command works with any type of instance.</li>
<li>The VM clock has no issues.</li>
</ul>

<p>The <code>nova live-migration</code> command has almost no instance downtime.</p>

<ul>
<li>The instance is suspended and does not see this as a shutdown.</li>
<li><p>The <code>live-migration</code> lets you specify the compute host you want to migrate to, however with some limitations.<br>
This requires shared storage, instances without a configdrive when block storage
is used, or volume-backed instances.</p></li>
<li><p>The migration fails if there are not enough resources on the target hypervisor</p></li>
<li><p>The VM clock might be off.</p></li>
</ul>

<p>Here are some examples when to use which option:</p>

<ul>
<li>If it is important to choose the compute host or to have very little downtime you need to use the <code>nova live-migration</code> command. </li>
<li>If you don&#39;t want to choose the compute host, or you have a configdrive enabled, you need to use the <code>nova migrate</code> command. </li>
<li>If you need to specify the compute host and you have a configdrive enabled, you need to manually migrate the machine, or use a dirty trick to fool <code>nova migrate</code>. </li>
</ul>

<p>All these options are described below in detail.</p>

<h3>Hypervisor Capacity</h3>

<p>Before you do a migration, check if the hypervisor host has enough free capacity
for the VM you want to migrate:</p>

<pre><code>nova host-describe compute-30
</code></pre>

<p>Example output:</p>

<pre><code>+-------------+----------------------------------+-----+-----------+---------+
| HOST        | PROJECT                          | cpu | memory_mb | disk_gb |
+-------------+----------------------------------+-----+-----------+---------+
| compute-30 | (total)                          | 64  | 512880    | 5928    |
| compute-30 | (used_now)                       | 44  | 211104    | 892     |
| compute-30 | (used_max)                       | 44  | 315568    | 1392    |
| compute-30 | 4[...]0288                       | 1   | 512       | 20      |
| compute-30 | 4[...]0194                       | 20  | 4506      | 62      |
+-------------+----------------------------------+-----+-----------+---------+
</code></pre>

<p>In this table, the first row shows the total amount of resources available on
the physical server. The second line shows the currently used resources. The
third line shows the maximum used resources. The fourth line and below shows the
resources available for each project.</p>

<p>If the VM flavor fits on this hypervisor, continue on with the manual migration.
If not, free up some resources or choose another compute server.</p>

<p>If the hypervisor node lacks enough capacity, the migration will fail.</p>

<h3>(Live) migration with nova live-migration</h3>

<p>The <code>live-migration</code> command works with the following types of vm&#39;s/storage:</p>

<ul>
<li>Shared storage: Both hypervisors have access to shared storage.</li>
<li>Block storage: No shared storage is required. Instances are backed by image-based root disks. Incompatible with read-only devices such as CD-ROMs and Configuration Drive (<code>config_drive</code>).</li>
<li>Volume storage: No shared storage is required. Instances are backed by iSCSI volumes rather than ephemeral disk.</li>
</ul>

<p>The live-migration command requires the same CPU on both hypervisors. It is
possible to set a generic CPU for the VM&#39;s, or a generic set of CPU features.
This however does not work on versions lower than Kilo due to <a href="https://bugs.launchpad.net/nova/+bug/1082414">a bug</a> where
Nova compares the actual CPU instead of the virtual CPU. In my case, all the
hypervisor machines are the same, lucky me. This is fixed in Kilo or later.</p>

<p>On versions older than Kilo, the Compute service does not use libvirt&#39;s live
migration functionality by default, therefore guests are suspended before
migration and might experience several minutes of downtime. This is because
there is a risk that the migration process will never end. This can happen if
the guest operating system uses blocks on the disk faster than they can be
migrated.</p>

<p>To enable true live migration using libvirt&#39;s migrate functionality, see the
Openstack documentation linked below.</p>

<h4>Shared storage / Volume backed instances</h4>

<p>A live-migration is very simple. Use the following command with an instance UUID
and the name of the compute host:</p>

<pre><code>nova live-migration $UUID $COMPUTE-HOST
</code></pre>

<p>If you have shared storage, or if the instance is volume backed, this will send
the instances memory (RAM) content over to the destination host. The source
hypervisor keeps track of which memory pages are modified on the source while
the transfer is in progress. Once the initial bulk transfer is complete, pages
changed in the meantime are transferred again. This is done repeatedly with
(ideally) ever smaller increments.</p>

<p>As long as the differences can be transferred faster than the source VM dirties
memory pages, at some point the source VM gets suspended. Final differences are
sent to the target host and an identical machine started there. At the same time
the virtual network infrastructure takes care of all traffic being directed to
the new virtual machine. Once the replacement machine is running, the suspended
source instance is deleted. Usually the actual handover takes place so quickly
and seamlessly that all but very time sensitive applications ever notice
anything.</p>

<p>You can check this by starting a <code>ping</code> to the VM you are live-migrating. It
will stay online and when the VM is suspended and resumed on the target
hypervisor, the ping responses will take a bit longer.</p>

<h4>Block based storage (--block-migrate)</h4>

<p>If you don&#39;t have shared storage and the VM is not backed by a volume as root
disk (image based VM&#39;s) a live-migration requires an extra parameter:</p>

<pre><code>nova live-migration ----block-migrate $UUID $COMPUTE-HOST
</code></pre>

<p>The process is almost exactly the same as described above. There is one extra
step however. Before the memory contents is sent the disk content is copied
over, without downtime. When the VM is suspended, both the memory contents and
the disk contents (difference to the earlier copy) are sent over. The suspend
action takes longer and might be noticable as downtime.</p>

<p>The <code>--block-migrate</code> option is incompatible with read only devices such as ISO
CD/DVD drives and the Config Drive.</p>

<h3>Migration with nova migrate</h3>

<p>The <code>nova migrate</code> command shuts down an instance, copies over the disk to a
hypervisor with enough free resources, starts it up there and removes it from
the source hypervisor. The VM is shut down and will be down as long as the
copying. With a <code>migrate</code>, the Openstack cluster chooses an compute-service
enabled hypervisor with the most resources available. This works with any type
of instance, with any type of backend storage.</p>

<p>A <code>migrate</code> is even simpler than a <code>live-migration</code>. Here&#39;s the syntax:</p>

<pre><code>nova migrate $UUID
</code></pre>

<p>This is perfect for instances that are part of a clustered service, or when you
have scheduled and communicated downtime for that specific VM. The downtime is
dependent on the size of the disk and the speed of the (storage) network.</p>

<p><code>rsync</code> over ssh is used to copy the actual disk, you can test the speed
yourself with a few regular rsync tests, and combine that with the disksize to
get an indication of the migration downtime.</p>

<h3>Migrating to a speficic compute node, the dirty way</h3>

<p>As seen above, we cannot migrate virtual machines to a specific compute node if
the compute node does not have shared storage and the virtual machine has a
configdrive enabled. You can force the Openstack cluster to choose a specific
hypervisor by disabling the <code>nova-compute</code> service on all the other hypervisors.
The VM&#39;s will keep running on there, only new virtual machines and migrations
are not possible on those hypervisors.</p>

<p>If you have a lot of creating and removing of machines in your Openstack Cloud,
this might be a bad idea. If you use (Anti) Affinity Groups, vm&#39;s created in
there will also fail to start depending on the type of Affinity Group. See <a href="https://raymii.org/s/articles/Openstack_Affinity_Groups-make-sure-instances-are-on-the-same-or-a-different-hypervisor-host.html">my
article on Affinity Groups</a> for more info on those.</p>

<p>Therefore, use this option with caution. If we have 5 compute nodes,
<code>compute-30</code> to <code>compute-34</code> and we want to migrate the machine to <code>compute-34</code>,
we need to disable the <code>nova-compute</code> service on all other hypervisors.</p>

<p>First check the state of the cluster:</p>

<pre><code>nova service-list --binary nova-compute # or nova-conductor, nova-cert, nova-consoleauth, nova-scheduler
</code></pre>

<p>Example output:</p>

<pre><code>+----+--------------+--------------+------+----------+-------+----------------------------+-----------------+
| Id | Binary       | Host         | Zone | Status   | State | Updated_at                 | Disabled Reason |
+----+--------------+--------------+------+----------+-------+----------------------------+-----------------+
| 7  | nova-compute | compute-30   | OS1  | enabled  | up    | 2015-06-13T17:04:27.000000 | -               |
| 8  | nova-compute | compute-31   | OS2  | enables  | up    | 2015-06-13T17:02:49.000000 | -               |
| 9  | nova-compute | compute-32   | OS2  | enabled  | up    | 2015-06-13T17:02:50.000000 | None            |
| 10 | nova-compute | compute-33   | OS2  | enabled  | up    | 2015-06-13T17:02:50.000000 | -               |
| 11 | nova-compute | compute-34   | OS1  | disabled | up    | 2015-06-13T17:02:49.000000 | Migrations Only |
+----+--------------+--------------+------+----------+-------+----------------------------+-----------------+
</code></pre>

<p>In this example we have 5 compute nodes, of which one is disabled with reason
Migrations Only. In our case, before we started migrating we have enabled nova-
compute on that hypervisor and disabled it on all the other hypervisors:</p>

<pre><code>nova service-disable compute-30 nova-compute --reason &#39;migration to specific hypervisor the dirty way&#39;
nova service-disable compute-31 nova-compute --reason &#39;migration to specific hypervisor the dirty way&#39;
etc...
</code></pre>

<p>Now execute the <code>nova migrate</code> command. Since you&#39;ve disabled all compute
hypervisors except the target hypervisor, that one will be used as migration
target.</p>

<p>All new virtual machines created during the migration will also be spawned on
that specific hypervisor.</p>

<p>When the migration is finished, enable all the other compute nodes:</p>

<pre><code>nova service-enable compute-30 nova-compute
nova service-enable compute-31 nova-compute
etc...
</code></pre>

<p>In our case, we would disable the <code>compute-34</code> because it is for migrations
only.</p>

<p>This is a bit dirty and might cause problems if you have monitoring on the
cluster state or spawn a lot of machines all the time.</p>

<h3>Manual migration to a specific compute node</h3>

<p>As seen above, we cannot migrate virtual machines to a specific compute node if
the compute node does not have shared storage and the virtual machine has a
configdrive enabled. Since Openstack is just a bunch of wrappers around native
Linux tools, we can manually migrate the machine and update the Nova database
afterwards.</p>

<p>Do note that this part is specific to the storage you use. In this example we
use local storage (or, a local folder on an NFS mount not shared with other
compute nodes) and image-backed instances.</p>

<p>In my case, I needed to migrate an image-backed block storage instance to a non-
shared storage node, but the instance had a configdrive enabled. Disabling the
compute service everywhere is not an option, since the cluster was getting about
a hundred new VM&#39;s every 5 minutes and that would overload the hypervisor node.</p>

<p>This example manually migrates a VM from <code>compute-30</code> to <code>compute-34</code>. These
nodes are in the same network and can access one another via SSH keys based on
their hostname.</p>

<p>Shut down the VM first:</p>

<pre><code>nova stop $VM_UUID
</code></pre>

<p>Also detach any volumes:</p>

<pre><code>nova volume-detach $VM_UUID $VOLUME_UUID
</code></pre>

<p>Use the <code>nova show</code> command to see the specific hypervisor the VM is running on:</p>

<pre><code>nova show UUID | grep hypervisor
</code></pre>

<p>Example output:</p>

<pre><code>| OS-EXT-SRV-ATTR:hypervisor_hostname  | compute-30    |
</code></pre>

<p>Login to that hypervisor via SSH. Navigate to the folder where this instance is
located, in our case, <code>/var/lib/nova-compute/instances/$UUID</code>.</p>

<p>The instance is booted from an image based root disk, named <code>disk</code>. <code>qemu</code> in
our case diffs the root disk from the image the VM was created from. Therefore
the new hypervisor also needs that backing image. Find out which file is the
backing image:</p>

<pre><code>cd /var/lib/nova-compute/instances/UUID/
qemu-img info disk # disk is the filename of the instance root disk
</code></pre>

<p>Example output:</p>

<pre><code>  image: disk
  file format: qcow2
  virtual size: 32G (34359738368 bytes)
  disk size: 1.3G
  cluster_size: 65536
  backing file: /var/lib/nova-compute/instances/_base/d00[...]61
  Format specific information:
      compat: 1.1
      lazy refcounts: false 
</code></pre>

<p>The file <code>/var/lib/nova-
compute/instances/_base/d004f7f8d3f79a053fad5f9e54a4aed9e2864561</code> is the backing
disk. Note that the long filename is not a UUID but a checksum of the specific
image version. In my case it is a raw disk:</p>

<pre><code>qemu-img info /var/lib/nova-compute/instances/_base/d00[...]61
</code></pre>

<p>Example output:</p>

<pre><code>image: /var/lib/nova-compute/instances/_base/d00[...]61
file format: raw
virtual size: 8.0G (8589934592 bytes)
disk size: 344M
</code></pre>

<p>Check the target hypervisor for the existence of that image. If it is not there,
copy that file to the target hypervisor first:</p>

<pre><code>rsync -r --progress /var/lib/nova-compute/instances/_base/d00[...]61 -e ssh compute-34:/var/lib/nova-compute/instances/_base/d00[...]61
</code></pre>

<p>On the target hypervisor, set the correct permissions:</p>

<pre><code>chown nova:nova /var/lib/nova-compute/instances/_base/d00[...]61
</code></pre>

<p>Copy the instance folder to the new hypervisor:</p>

<pre><code>cd /var/lib/nova-compute/instances/
rsync -r --progress $VM_UUID -e ssh compute-34:/var/lib/nova-compute/instances/
</code></pre>

<p>Set the correct permissions on the folder on the target hypervisor:</p>

<pre><code>chown nova:nova /var/lib/nova-compute/instances/$VM_UUID
chown nova:nova /var/lib/nova-compute/instances/$VM_UUID/disk.info 
chown nova:nova /var/lib/nova-compute/instances/libvirt.xml

chown libvirt:kvm /var/lib/nova-compute/instances/$VM_UUID/console.log 
chown libvirt:kvm /var/lib/nova-compute/instances/$VM_UUID/disk 
chown libvirt:kvm /var/lib/nova-compute/instances/$VM_UUID/disk.config
</code></pre>

<p>If you use other usernames and groups, change those in the command.</p>

<p>Log in to your database server. In my case that is a MySQL Galera cluster. Start
up a MySQL command prompt in the <code>nova</code> database:</p>

<pre><code>mysql nova
</code></pre>

<p>Execute the following command to update the <code>nova</code> database with the new
hypervisor for this VM:</p>

<pre><code>update instances set node=&#39;compute-34&#39;, host=node where uuid=&#39;$VM_UUID&#39;;
</code></pre>

<p>This was tested on an <code>IceHouse</code> database scheme, other versions might require
other queries.</p>

<p>Use the <code>nova show</code> command to see if the new hypervisor is set. If so, start
the VM:</p>

<pre><code>nova start $VM_UUID
</code></pre>

<p>Attach any volumes that were detached earlier:</p>

<pre><code>nova volume-attach $VM_UUID $VOLUME_UUID
</code></pre>

<p>Use the console to check if it all works:</p>

<pre><code>nova get-vnc-console $VM_UUID novnc
</code></pre>

<p>Do note that you must check the free capacity yourself. The VM will work if
there is not enough capacity, but you do run in to weird issues with the
hypervisor like bad performance or killed processes (OOM&#39;s).</p>

<h3>Conclusion</h3>

<p>Openstack offers many ways to migrate machines from one compute node to another.
Each way is applicable in certain scenario&#39;s, and if all else fails you can
manually migrate machines using the underlying linux tools. This article gives
you an overview of the most common migration ways and the scenario&#39;s when they
are applicable. Happy migrating.</p>

<h3>Further reading</h3>

<ul>
<li><a href="http://docs.openstack.org/admin-guide-cloud/content/section_configuring-compute-migrations.html">http://docs.openstack.org/admin-guide-cloud/content/section_configuring-compute-migrations.html</a></li>
<li><a href="http://docs.openstack.org/admin-guide-cloud/content/section_live-migration-usage.html">http://docs.openstack.org/admin-guide-cloud/content/section_live-migration-usage.html</a></li>
</ul>
Tags: <a href="../tags/articles.html" class="link">articles</a>, <a href="../tags/cloud.html" class="link">cloud</a>, <a href="../tags/cluster.html" class="link">cluster</a>, <a href="../tags/compute.html" class="link">compute</a>, <a href="../tags/kvm.html" class="link">kvm</a>, <a href="../tags/migrate.html" class="link">migrate</a>, <a href="../tags/nova.html" class="link">nova</a>, <a href="../tags/openstack.html" class="link">openstack</a>, <a href="../tags/qemu.html" class="link">qemu</a></div><br/><footer><br/>
                <form role="search" action="https://encrypted.google.com/search"
                style="min-width:250px;max-width:300px;">
                    <div class="form-group">
                      <input type="hidden" name="as_sitesearch" value="raymii.org">
                      <input type="hidden" name="as_qdr" value="all">
                      <input type="text" name="as_q" class="form-control" placeholder="Search">
                    </div>
                  </form>
                <br>
                <p><small>
                <a href="/s/">Home</a> | 
                <a href="/s/static/About.html">About</a> | 
                <a href="/s/tags/all.html">All pages</a> | 
                <a href="/s/software/Sparkling_Network.html">Cluster Status</a> | 
                Generated by <a href="/s/software/ingsoc.html">ingsoc</a>.</small></p>
            
    </footer>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-3704876-6"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-3704876-6');
    </script>
     
    </body>
    </html>
    